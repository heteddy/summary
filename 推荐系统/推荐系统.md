# 1推荐系统和搜索的区别

- 搜索：当明确自己的目标的时候，用搜索引擎解决问题
- 推荐：当目标不明确的时候，用推荐系统推荐解决

#  怎么实现推荐

推荐系统根据用户和物品，给用户推荐相似的物品，可以分为以下几类：

+ I2I （item to item）

  一个物品到另外一个物品

+ U2I (user to item)

+ U2I2I (user to item to item)

+ U2U2I (user to user to item)

+ U2TAG2I (user to tag to item)

回到如何实现推荐系统，可以根据用户历史记录或者物品信息与数据库中的所有数据进行**<font size="5" color="red">相似度</font>**计算, 计算出相似度最高的物品, 推荐给用户。因此引出2个问题：

1. <font color="red">怎么计算相似度</font>
2. <font color="red">与数据库中的所有物品全部计算一遍耗时太久</font>

# 流程

整个执行流程主要包括召回和排序两个步骤，排序又可以分为粗排和精排，主要区别是使用的算法复杂度以及处理的数据量。

**召回**: 主要根据用户和商品部分特征，从海量的物品库里，快速找回一小部分用户潜在感兴趣的物品，然后交给排序环节.



![数据流.png](https://upload-images.jianshu.io/upload_images/9243349-b959141ba4dfdede.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 怎么计算相似度

计算相似度依赖相似度算法：

+ 余弦相似度

  计算夹角

+ jaccard相似度

  公式：交集除以并集 

  **佛学推荐系统，文本聚类使用了jaccard计算相似度**

+ 皮尔逊相关系数

<font color="red">这些算法可以计算，但是物品和用户信息怎么做输入参数呢？</font>

# 算法的输入--特征化

## one-hot编码

每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。<font color="red">哪些信息可以转为特征？</font>

缺点：

1. 如果文本数量很大，不是一句话，而是一本长篇小说，那需要表示成为一个...无法想象的矩阵，那这样的矩阵会过于稀疏，过度占用资源
2. 映射之间完全独立，并不能表示出不同类别之间的关系

<font color="red">为了解决以上缺点引入embedding。</font>

one-hot向量比较稀疏适用于类别等编码，比如性别，国籍，年龄区间分类等；

## multi-hot

multi-hot编码之后每个id对应的是多个的1，而且不同样本中1的个数还不一样, 常用于用户画像或为用户做兴趣标签

## embedding

embedding把高维的稀疏特征转换为低维的稠密向量，而且能综合多种信息和特征（通过全连接层实现）。

embedding向量的性质是能使距离相近的向量对应的物体有相近的含义, 下图左边的例子，从 king 到 queen 的向量和从 man 到 woman 的向量，无论从方向还是尺度来说它们都异常接近。这说明词 Embedding 向量间的运算居然能够揭示词之间的性别关系,比如 woman 这个词的词向量可以用下面的运算得出：

`Embedding(woman)=Embedding(man)+[Embedding(queen)-Embedding(king)]`

<img src="https://static001.geekbang.org/resource/image/19/0a/19245b8bc3ebd987625e36881ca4f50a.jpeg" style="zoom:50%;" />



<font color="red">embedding的主要用途是作为召回层，解决相似物品的召回问题</font>,通过物品向量的內积计算得到。

![](https://bkimg.cdn.bcebos.com/formula/a71fd9091b7164c710ef5f590a71f6ad.svg)

但是在召回过程中，如果物品数量比较多几百万，那么实现U2I或者I2I的时间复杂度是 百万*百万，导致线上运行的延迟太高，那么<font color="red">如何提高响应时间?</font>,引入局部敏感hash和faiss。

**word2vector** 是一个计算词嵌入/词向量（word embedding）的工具，包含两种训练模型：

+ CBOW模型根据中心词w(t)周围的词如w(t-2)&w(t-1)&w(t+1)&w(t+2)来预测中心词w(t)；

+ Skip-gram模型则根据中心词W(t)来预测周围词。

<font color="red">word2vector是怎么算出来的？</font>

# 可向量化的信息

下列信息进行特征化后作为模型的输入，主要包括：显式反馈数据（评分，播放），隐式反馈数据（转发，停留时长）。

## 用户行为信息

+ 评分
+ 评论
+ 点赞，踩
+ 收藏
+ 点击
+ 播放时长
+ 上线频率
+ 在线时长

## 用户关系数据

+ 好友

## 属性标签数据

​		标签属性类数据与其描述主体一起构建成知识图谱(Knowledge Graph),在其上施以 Graph Embedding 或者 GNN（Graph Neural Network，图神经网络）生成各节点的 Embedding

+ 性别年龄住址
+ 注册信息，来源
+ 兴趣标签
+ 物品属性，类别

## 内容数据

+ 文字
+ 图片

## 场景信息

+ 时间

+ gps IP地址 地点

+ 季节

+ 天气

+ 社会事件（斋戒日）

  

浏览历史，点击日志等这些数据往往只有**正反馈**而没有**负反馈**，并且数据是高度稀疏的（相比于用户点击过的产品，没有被点击过的产品的数量非常多）。在推荐系统中使用隐式反馈数据，还有两种学习方法：1）负采样，2）非采样。

## 负采样 Negative Sampling

负采样通常采用的策略skip-above即推荐给用户的物品中用户点击的为正采样，被点击物品之上的物品为负采样（因为物品是从上到下顺序展示，发生一次点击就说明顺序看到了当前物品）。

**skip-above增强：**

1. 在用户未点击的部分，选择流行度高的作为负样本（更有代表性）
2. 在用户未点击的部分，删除用户近期已发生观看
3. 在用户未点击的部分，统计相应的曝光数据，取Top作为负样本（多次曝光仍无转化）

## 非采样 Non-Sampling

将所有用户都未标记的样例作为负例

https://www.jiqizhixin.com/articles/2020-02-19-4

# 为什么需要分词

两个目的，对于搜索由于使用的是反向索引，分词(相比字)可以降低索引的存储空间和搜索的时间

从语义上讲，分词可以切分上下文的耦合，降低词序的影响

# 为什么要做NER

**命名实体识别(NER)**的任务是找到文本中提到的每个命名实体，并标记其类型。构成命名实体类型的是特定于任务的;人员、地点和组织。一旦提取了文本中的所有命名实体，就可以将它们链接到与实际实体相对应的集合中。

**关系抽取**：发现和分类文本实体之间的语义关系。这些关系通常是二元关系，如子女关系、就业关系、部分-整体关系和地理空间关系。

# 关键词

关键词是一个句子或者问章中比较重要的词   （目前我们系统中关键词算法的一种实现tf-idf）

+ tf-idf 

![](https://ruanyifeng.com/blogimg/asset/201303/bg2013031507.png)

​			tf：词频

​			idf:**逆文档频率**

​			每个词分配一个"重要性"权重。最常见的词（"的"、"是"、"在"）给予最小的权重，较常见的词（"中国"）给予较小的权重，较少见的词（"蜜蜂"、"养殖"）给予较大的权重。这个权重叫做"逆文档频率"（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。

# word2vector怎么计算的

​		它的输入层和输出层的维度都是 V，这个 V 其实就是语料库词典的大小。假设语料库一共使用了 10000 个词，那么 V 就等于 10000。根据图 4 生成的训练样本，这里的输入向量自然就是由输入词转换而来的 One-hot 编码向量，输出向量则是由多个输出词转换而来的 Multi-hot 编码向量，显然，基于 Skip-gram 框架的 Word2vec 模型解决的是一个多分类问题。

​		把输入向量矩阵转换成词向量查找表，例如，输入向量是 10000 个词组成的 one hot 向量，隐层维度是 300 维，那么输入层到隐层的权重矩阵为 10000x300 维。在转换为词向量 Lookup table 后，每行的权重即成了对应词的 Embedding 向量

<img src="https://static001.geekbang.org/resource/image/0d/72/0de188f4b564de8076cf13ba6ff87872.jpeg" style="zoom:50%;" />



# 召回和排序区别

召回数据量比较大，使用算法要求速度快，模型简单，特征比较少

排序数据量小，使用算法模型复杂，特征比较多，计算比较准确

# 多路召回

采用不同的策略和特征分别召回一部分候选集，然后把候选集合混在一起交给排序模型，多路召回是在`计算速度`和`召回率`之间的一个平衡，采用简单策略从不同角度快速召回。

# 推荐方法

## 协同过滤

<img src="https://static001.geekbang.org/resource/image/39/42/3960001f6c049652160cb16ff3ddee42.jpg" style="zoom:40%;" />

+ userCF

  用基于物品的喜好或者评分矩阵表示用户

  缺点：

  1. 用户数大于物品数，需要维护大量的用户矩阵找到topn 相似用户，如果为每个用户存储top n，那么在线存储的增长压力太大
  2. 用户的历史数据比较稀疏，不适用于反馈获取比较困难的场景，比如酒店，大件商品的购买记录

+ itemCF

  基于物品相似度进行推荐。通过计算物品向量的相似度得到物品之间的相似矩阵。

  缺点：热门物品具有很强的头部效应，容易跟大量的物品产生相似性，尾部的物品由于特征向量比较稀疏，很少与其他物品产生相似性，导致很少被推荐（<font color="red">泛化性不好</font>）

无法有效的引入用户年龄，性别，商品描述，商品分类，当前的时间等用户特征物品特征，以及上下文信息。

## 矩阵分解

​		<font color="red">增加模型的泛化能力，使用更稠密的隐向量表示用户和物品，挖掘用户的物品的隐含兴趣和隐含特征，在一定程度上弥补协同过滤处理稀疏矩阵能力不足的问题。</font>

​		矩阵分解的主要过程，就是先分解协同过滤生成的共现矩阵，生成用户和物品的隐向量，再通过用户和物品隐向量间的相似性进行推荐。矩阵分解的过程就是把一个 mxn 的共现矩阵，分解成一个 mxk 的用户矩阵和 kxn 的物品矩阵相乘的形式,有了用户矩阵和物品矩阵，用户隐向量和物品隐向量就非常好提取了。用户隐向量就是用户矩阵相应的行向量，而物品隐向量就是物品矩阵相应的列向量。

![](https://static001.geekbang.org/resource/image/60/fb/604b312899bff7922528df4836c10cfb.jpeg)

## 逻辑回归

逻辑回归可以综合利用用户物品,上下文等不同的特征，

是将推荐问题定义为一种分类问题即通过预测正样本的概率作为排序依据，把推荐问题转换为一个CTR预估的问题。

## 基于深度神经网络

TBD(德涛补充)

![](https://static001.geekbang.org/resource/image/10/c5/10e8105911823d96348dc7288d4d26c5.jpg)

### wide&deep

![](https://static001.geekbang.org/resource/image/fb/e0/fb17112c951ebb2a515f12dace262de0.jpg)

模型是由 Google 的应用商店团队 Google Play 提出的,模型是由左侧的 Wide 部分和右侧的 Deep 部分组成的。Wide 部分的结构简单就是把输入层直接连接到输出层，中间没有做任何处理。

Wide 部分的主要作用是让模型具有较强的“记忆能力”（Memorization），而 Deep 部分的主要作用是让模型具有“泛化能力”（Generalization），因为只有这样的结构特点，才能让模型兼具逻辑回归和深度神经网络的优点，也就是既能快速处理和记忆大量历史行为特征，又具有强大的表达能力，这就是 Google 提出这个模型的动机。

所谓的 “记忆能力”，可以被宽泛地理解为模型直接学习历史数据中物品或者特征的“共现频率”，并且把它们直接作为推荐依据的能力。

“泛化能力”指的是模型对于新鲜样本、以及从未出现过的特征组合的预测能力。

# 如何提高性能-响应时间

前面已经说过处于同一个向量空间内的用户和所有物品计算相似度数据量太大，而召回问题可以转化为在一个多维向量空间内寻找近邻的过程。而要解决近邻搜索有两种不同的方式，都与服务器端的解决高并发的思路接近；<font color="red">一种是使用hash分桶，另外一种是加索引</font>

1. 局部敏感哈希

   让相邻的点落入到同一个桶中，这样在近邻搜索的过程中仅需要找到对应的桶或者相邻的桶。

   **分桶的理论依据：欧式空间中，将高维空间的点映射到低维空间，原本接近的点在低维空间中肯定依然接近，但原本远离的点则有一定概率变成接近的点**

   

2. faiss

   faiss的核心就是索引（index）通过建立向量距离的索引，查找近邻的时候，通过索引缩小范围。faiss中由多种类型的索引，最简单的索引类型：indexFlatL2是暴力检索L2距离（欧式距离），在建立的时候都包含了训练阶段，但是L2这个索引可以跳过。当索引被建立 和训练之后，就可以调用add，search着两种方法。

   faiss index 类型

   `IndexFlatL2` 暴力检索L2距离（欧式距离）

   `IndexIVFFlat`（倒排文件），起始就是使用k-means建立聚类中心，然后通过查询最近的聚类中心，然后比较聚类中所有向量得到相似的向量。

    `IndexIVFPQ`基于Product Quantizer对高维向量进行压缩，降低内存占用

   https://github.com/facebookresearch/faiss



# 冷启动

+ 用户冷启动
+ 物品冷启动

# AB测试

将用户随机分成实验组和对照组，实验组使用新模型，对照组使用旧模型，比较线上评估指标(CTR等)的差异

+ 抖音的分级流量池机制

# 推荐架构

推荐服务分为三个层次：

1. 在线服务
2. 近线计算，使用spark stream或flink
3. 离线计算

分级存储：

1. 大数据 hdfs  - Hive   （亿级）
2. mongodb 等非关系数据库 （千万）
3. redis 

![架构.png](https://upload-images.jianshu.io/upload_images/9243349-85005c0ee27c4616.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 如何评价推荐效果

**准确率**(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) = 70%
**精确率**(precision) = TP/(TP+FP) = 80%
**召回率**(recall) = TP/(TP+FN) = 2/3

- TP: 将正类预测为正类数  40
- FN: 将正类预测为负类数  20
- FP: 将负类预测为正类数  10
- TN: 将负类预测为负类数  30

