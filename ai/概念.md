# 回归
[线性回归模型的概念、原理、代码和应用](https://zhuanlan.zhihu.com/p/80887841)

## 概念

回归是一种用于数值预测的技术，是一种统计学方法, **回归分析统计方法研究变量之间的关系并对其构建模型。**比如：**子辈的平均身高是其父辈平均身高以及他们所处族群平均身高的加权平均和**。**回归模型更像是显示了两个变量的统计关联度**

1. 回归（regression）是一个监督学习（有target的学习）

2. 回归用于预测输入和输出变量的关系，回归问题等价于函数拟合

3. 回归分为学习和预测过程，学习样本规律构建model，预测系统验证学习的model

## 分类
### 线性回归

线性关系，类似2维空间的直线，用一个方程式来表示它，即`Y=a+b*X + e`(Y是因变量，X是自变量)，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。

一元线性回归和多元线性回归的区别在于，多元线性回归有（>1）个自变量(x)，而一元线性回归通常只有1个自变量。现在的问题是“我们如何得到一个最佳的拟合线呢？”

**如何获得最佳拟合线（a和b的值）？**最小二乘法

它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。

1. 概念/过程
   + 损失函数
   + 
2. 基本过程

**要点：**

- 自变量与因变量之间必须有线性关系
- 多元回归存在多重共线性，自相关性和异方差性。
- 线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。
- 多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定
- 在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。

### 逻辑回归

逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。这里，Y的值从0到1，它可以用下方程表示。

### **多项式回归**

对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：

`y=a+b*x^2`

在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。



## 过拟合overfit和欠拟合underfit

### 训练误差

### 泛华误差



### 概念

+ overfit（训练量不足）

  训练误差远小于测试数据集的误差

+ underfit（）

  无法得到较低的训练误差（模拟考试很差，真实也很差）

# 卷积神经网络

## 卷积

4\*4 转换为2\*2个单元，每个单元 3\*3

[如何通俗易懂地解释卷积](https://www.zhihu.com/question/22298352)

## 池化层（pooling）

+ max
+ avg

## 多层感知机



# 深度卷积神经网络

# 批量规划

# ReLU 函数

**线性整流函数**（Rectified Linear Unit, **ReLU**），又称**修正线性单元，**是一种[人工神经网络](https://baike.baidu.com/item/人工神经网络)中常用的激活函数（activation function），通常指代以[斜坡函数](https://baike.baidu.com/item/斜坡函数)及其变种为代表的非线性函数。

# 循环神经网络

序列计算概率

解决k-independent 马尔科夫链，即使是k个相互关联，数据还是太大，计算比较

## 梯度计算

优化算法需要梯度，

## 反向传播

反向传播

## 正向传播

# 门控循环单元（GRU）

## R 重置门 gate

+ 捕捉时序数据短期的依赖关系

## Z 更新门

+ 捕捉时序数据中长期的依赖关系

# LSTM 长短期记忆（long short-term memory）



## 输入门 I

## 遗忘门 F

F和I是0~1之间

## 输出门



# 余弦相似度

捕捉2个词之间的相似度

# word2vec

## 模型

### 跳字模型 skip-gram

### 连续词袋模型 CBOW(continuous bag of words ) 

## 训练

### 负采样

### 层序









