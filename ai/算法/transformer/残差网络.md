ResNet（Residual Neural Network）由微软研究院的Kaiming He等四名华人提出，通过使用ResNet Unit成功训练出了152层的神经网络，并在ILSVRC2015比赛中取得冠军，在top5上的错误率为3.57%，同时参数量比VGGNet低，效果非常突出。ResNet的结构可以极快的加速神经网络的训练，模型的准确率也有比较大的提升。同时ResNet的推广性非常好，甚至可以直接用到InceptionNet网络中。



下图是ResNet34层模型的结构简图。

![](https://i-blog.csdnimg.cn/blog_migrate/9a66b0796ab3023145996e779411cd10.png)



2、ResNet详解

在ResNet网络中有如下几个亮点：



提出residual结构（残差结构），并搭建超深的网络结构(突破1000层)

使用Batch Normalization加速训练(丢弃dropout)

在ResNet网络提出之前，传统的卷积神经网络都是通过将一系列卷积层与下采样层进行堆叠得到的。但是当堆叠到一定网络深度时，就会出现两个问题。



梯度消失或梯度爆炸。

退化问题(degradation problem)。

在ResNet论文中说通过数据的预处理以及在网络中使用BN（Batch Normalization）层能够解决梯度消失或者梯度爆炸问题。如果不了解BN层可参考这个链接

。但是对于退化问题（随着网络层数的加深，效果还会变差，如下图所示）并没有很好的解决办法。

![](https://i-blog.csdnimg.cn/blog_migrate/1119ecb74f13a77af53c269f9a4b017b.png)



所以ResNet论文提出了residual结构（残差结构）来减轻退化问题。下图是使用residual结构的卷积网络，可以看到随着网络的不断加深，效果并没有变差，反而变的更好了。

![](https://i-blog.csdnimg.cn/blog_migrate/b1a5b16f07e1fbad0afc1091e4a614d7.png)

残差结构（residual）



残差指的是什么？

其中ResNet提出了两种mapping：一种是identity mapping，指的就是下图中”弯弯的曲线”，另一种residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x



identity mapping

顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。



下图是论文中给出的两种残差结构。左边的残差结构是针对层数较少网络，例如ResNet18层和ResNet34层网络。右边是针对网络层数较多的网络，例如ResNet101，ResNet152等。为什么深层网络要使用右侧的残差结构呢。因为，右侧的残差结构能够减少网络参数与运算量。同样输入一个channel为256的特征矩阵，如果使用左侧的残差结构需要大约1170648个参数，但如果使用右侧的残差结构只需要69632个参数。明显搭建深层网络时，使用右侧的残差结构更合适。

![](https://i-blog.csdnimg.cn/blog_migrate/d678a4227d299b0d1d8e8e24d81294db.png)



我们先对左侧的残差结构（针对ResNet18/34）进行一个分析。



如下图所示，该残差结构的主分支是由两层3x3的卷积层组成，而残差结构右侧的连接线是shortcut分支也称捷径分支（注意为了让主分支上的输出矩阵能够与我们捷径分支上的输出矩阵进行相加，必须保证这两个输出特征矩阵有相同的shape）。如果刚刚仔细观察了ResNet34网络结构图的同学，应该能够发现图中会有一些虚线的残差结构。在原论文中作者只是简单说了这些虚线残差结构有降维的作用，并在捷径分支上通过1x1的卷积核进行降维处理。而下图右侧给出了详细的虚线残差结构，注意下每个卷积层的步距stride，以及捷径分支上的卷积核的个数（与主分支上的卷积核个数相同）。

![](https://i-blog.csdnimg.cn/blog_migrate/4e654ad0f0d7a9341abffd62a9ec8bd3.png)



接着我们再来分析下针对ResNet50/101/152的残差结构，如下图所示。在该残差结构当中，主分支使用了三个卷积层，第一个是1x1的卷积层用来压缩channel维度，第二个是3x3的卷积层，第三个是1x1的卷积层用来还原channel维度（注意主分支上第一层卷积层和第二次卷积层所使用的卷积核个数是相同的，第三次是第一层的4倍）。该残差结构所对应的虚线残差结构如下图右侧所示，同样在捷径分支上有一层1x1的卷积层，它的卷积核个数与主分支上的第三层卷积层卷积核个数相同，注意每个卷积层的步距。

![](https://i-blog.csdnimg.cn/blog_migrate/60f59e0bc2f7645a13fb747766f48ab5.png)



为什么残差学习相对更容易，从直观上看残差学习需要学习的内容少，因为残差一般会比较小，学习难度小点。不过我们可以从数学的角度来分析这个问题，首先残差单元可以表示为：

![](https://i-blog.csdnimg.cn/blog_migrate/6dcf100286408247a655dbfd85a7e693.png)





其中 XL和 XL+1分别表示的是第L个残差单元的输入和输出，注意每个残差单元一般包含多层结构。 F是残差函数，表示学习到的残差，而 h(XL)=XL表示恒等映射， F是ReLU激活函数。基于上式，我们求得从浅层 l到深层 L 的学习特征为：

![](https://i-blog.csdnimg.cn/blog_migrate/423b759d834542d724efdee4a7209412.png)



式子的第一个因子表示的损失函数到达L的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。要注意上面的推导并不是严格的证明。



下面这幅图是原论文给出的不同深度的ResNet网络结构配置，注意表中的残差结构给出了主分支上卷积核的大小与卷积核个数，表中的xN表示将该残差结构重复N次。那到底哪些残差结构是虚线残差结构呢。

![](https://i-blog.csdnimg.cn/blog_migrate/d65fdf1592b145828a769effab2bdf9d.png)



对于我们ResNet18/34/50/101/152，表中conv3_x, conv4_x, conv5_x所对应的一系列残差结构的第一层残差结构都是虚线残差结构。因为这一系列残差结构的第一层都有调整输入特征矩阵shape的使命（将特征矩阵的高和宽缩减为原来的一半，将深度channel调整成下一层残差结构所需要的channel）。为了方便理解，下面给出了ResNet34的网络结构图，图中简单标注了一些信息。

![](https://i-blog.csdnimg.cn/blog_migrate/d2a92a16242e41eb4128fae07d49b8d1.png)

对于我们ResNet50/101/152，其实在conv2_x所对应的一系列残差结构的第一层也是虚线残差结构。因为它需要调整输入特征矩阵的channel，根据表格可知通过3x3的max pool之后输出的特征矩阵shape应该是[56, 56, 64]，但我们conv2_x所对应的一系列残差结构中的实线残差结构它们期望的输入特征矩阵shape是[56, 56, 256]（因为这样才能保证输入输出特征矩阵shape相同，才能将捷径分支的输出与主分支的输出进行相加）。所以第一层残差结构需要将shape从[56, 56, 64] --> [56, 56, 256]。注意，这里只调整channel维度，高和宽不变（而conv3_x, conv4_x, conv5_x所对应的一系列残差结构的第一层虚线残差结构不仅要调整channel还要将高和宽缩减为原来的一半）。



# 代码

```python
import torch
from torch import nn
from torch.nn import functional as F

class Residual_block(nn.Module):
    def __init__(self,input_channels,output_channels,first=False):
        super().__init__()
        self.first=first
        if first==True:
            self.conv1=nn.Conv2d(input_channels,output_channels,stride=2,kernel_size=3,padding=1)
            self.conv3=nn.Conv2d(input_channels,output_channels,kernel_size=1,stride=2)
        else:
            self.conv1=nn.Conv2d(output_channels,output_channels,kernel_size=3,padding=1)
        self.bn1=nn.BatchNorm2d(output_channels)
        self.conv2=nn.Conv2d(output_channels,output_channels,kernel_size=3,padding=1)
        self.bn2=nn.BatchNorm2d(output_channels)
        
    def forward(self,x):
        Y=F.relu(self.bn1(self.conv1(x)))
        Y=self.bn2(self.conv2(Y))
        if self.first==True:
            x=self.conv3(x)
        Y=x+Y   // 这里是实现残差
        return F.relu(Y)
        
        
def resnet_block(input_channels,output_channels,num_residual_block,special=False):
    blk=[]
    for i in range(num_residual_block):
        if i==0 and special==True:
            blk.append(Residual_block(input_channels,input_channels))
        if i==0 and special==False:
            blk.append(Residual_block(input_channels,output_channels,first=True))
        else:
            blk.append(Residual_block(output_channels,output_channels))
    return blk


b1=nn.Sequential(
    nn.Conv2d(kernel_size=7,in_channels=3,out_channels=64,stride=2,padding=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3,stride=2,padding=1),
)
R1=Residual_block(64,64)
x=torch.ones(1,3,224,224)
for layer in b1:
    x=layer(x)

b18_2=nn.Sequential(*resnet_block(64,64,2,special=True))
b18_3=nn.Sequential(*resnet_block(64,128,2))
b18_4=nn.Sequential(*resnet_block(128,256,2))
b18_5=nn.Sequential(*resnet_block(256,512,2))

b34_2=nn.Sequential(*resnet_block(64,64,3,special=True))
b34_3=nn.Sequential(*resnet_block(64,128,4))
b34_4=nn.Sequential(*resnet_block(128,256,6))
b34_5=nn.Sequential(*resnet_block(256,512,3))


#Resnet-18
Resnet_18=nn.Sequential(b1,b18_2,b18_3,b18_4,b18_5,nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),nn.Linear(512,10))
#Resnet-34
Resnet_18=nn.Sequential(b1,b34_2,b34_3,b34_4,b34_5,nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),nn.Linear(512,10))
```