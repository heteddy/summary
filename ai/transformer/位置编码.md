

***





猫抓老鼠  

老鼠抓猫

# 什么是位置编码

# 如何计算位置编码





$PE_(pos,2i)=\sin\Big(\frac{pos}{10000^\frac{2i}{d_{model}}}\Big)$

$PE_(pos,2i+1) = \cos\Big(\frac{pos}{10000^\frac{2i}{d_{model}}}\Big)$

# 真正的意义

训练的就是 "词向量+位置信息"的新特征，极大丰富了输入特征

$x_i = w+p_i$

- 问题

  当 $a+x = b+y$ 时候数学上称为碰撞，不同的词向量和位置编码组合的结果恰好是相同的向量表示；

  即使某个维度出现碰撞，所有维度都出现碰撞的概率非常低

- 

# 代码实现









transformer 的位置编码作用是什么，推理过程input能无限长吗

### 问题一：Transformer的位置编码作用是什么？

**核心答案：** 位置编码的作用是**为模型提供序列中各个词的位置（顺序）信息**。

**详细解释：**

1. **Transformer的先天缺陷：自注意力机制的“无序性”**  
   Transformer的核心是自注意力机制。自注意力机制通过计算所有词对之间的关联度来加权求和，从而得到每个词的上下文感知表示。  
   然而，自注意力机制本身是**排列不变** 的。这意味着，如果你把输入序列的词序完全打乱，自注意力层输出的结果（在内容上）虽然会改变，但不会因为顺序本身而改变。它无法区分以下两个句子：

   - “猫抓住了老鼠。”
   - “老鼠抓住了猫。”  
     因为对于自注意力来说，它只关心“猫”和“老鼠”这两个词出现了，以及它们之间的关联强度，但无法获知谁在前谁在后。

2. **位置编码作为“补救措施”**  
   为了解决这个问题，我们需要一种方式将词的**绝对或相对位置信息**明确地注入到模型中。位置编码就是做这个的。

   - 它在输入序列的每个词嵌入向量上，**叠加**一个特定的、能表征其位置信息的向量。
   - 这个叠加了位置信息的向量再被送入Transformer的编码器或解码器。

   **公式化表示：**  
`最终输入 = 词嵌入向量 + 位置编码向量`

3. **位置编码是如何工作的？（以原始Transformer的Sinusoidal编码为例）**  
   原始论文使用了一组正弦和余弦函数来生成位置编码：

   PE(pos,2i)=sin⁡(pos/100002i/dmodel)PE(pos,2i)​=sin(pos/100002i/dmodel​)  
PE(pos,2i+1)=cos⁡(pos/100002i/dmodel)PE(pos,2i+1)​=cos(pos/100002i/dmodel​)

   其中：

   - `pos`：词在序列中的位置（0, 1, 2, ...）
   - `i`：位置编码向量的维度索引（0, 1, 2, ..., d_model/2）
   - `d_model`：词嵌入向量的维度（也是位置编码的维度）



   **这种设计有几个精妙之处：**

   - **可以表示任意长度的位置**：由于使用的是周期性的正弦和余弦函数，它可以轻松地为比训练时见过的任何序列都长的序列生成位置编码。
   - **蕴含相对位置信息**：某个位置 `pos + k` 的位置编码可以通过位置 `pos` 的位置编码进行线性变换得到，这使得模型能够很容易地学习到相对位置信息。
   - **值域有界**：正弦余弦函数的值在[-1, 1]之间，与词嵌入向量的尺度相匹配。

4. **其他类型的位置编码**

   - **可学习的位置编码**：比如BERT模型，它将位置编码视为可学习的参数，在训练过程中随模型一起优化。这种方式更简单，但可能缺乏外推到更长序列的能力。
   - **相对位置编码**：不再关注词的绝对位置，而是关注词与词之间的相对距离（如“相邻”、“隔一个词”等）。这在一些现代模型（如T5, DeBERTa）中表现更好。


**总结：** 没有位置编码，Transformer就是一个“词袋”模型，无法理解语言的顺序结构。位置编码是Transformer能够处理序列任务（如翻译、文本生成）的**关键组件**。

***

### 问题二：推理过程input能无限长吗？

**核心答案：** **不能无限长**。推理时的输入长度受到严格的限制。

**详细解释：**

1. **训练时固定长度的限制**  
   Transformer模型在训练时，出于计算效率和内存管理的考虑，通常会在一个固定的最大序列长度（例如512或1024）上进行训练。模型的参数（特别是位置编码和注意力权重矩阵）是在这个预设的长度上学习和适应的。

   - **对于Sinusoidal编码**：虽然理论上它可以计算任意长的位置编码，但模型的**注意力机制**和**全连接层**的权重是在训练长度上学习到的。当序列远长于训练长度时，模型就像进入了一个“未知领域”，其权重可能无法有效处理这种新的数据分布，导致性能急剧下降。这种现象被称为**长度外推能力差**。
   - **对于可学习的位置编码**：问题更严重。模型只学习了前N个（比如512个）位置向量的参数，对于第513个及以后的位置，它根本没有对应的编码向量。

2. **计算复杂度的限制**  
   自注意力机制的计算复杂度是 O(n2)O(n2)，其中 nn 是序列长度。

   - 当序列长度翻倍时，计算所需的内存和时间会变为原来的四倍。
   - 因此，从硬件和计算资源的角度看，无限长的序列也是不现实的。

3. **实际中的“长文本”处理**  
   虽然不能“无限长”，但学术界和工业界一直在努力**扩展上下文长度**。以下是一些解决方案：

   - **窗口滑动/分块处理**：将长文本切割成多个符合模型最大长度的块，分别处理后再合并结果。这是一种常用的工程方法，但会丢失块与块之间的上下文信息。
   - **改进的位置编码**：设计具有更好外推能力的位置编码方案。

     - **ALiBi**：在注意力分数上直接添加一个与相对距离成负相关的偏置项，被证明具有极强的外推能力。
     - **RoPE**：通过旋转矩阵将位置信息注入到查询和键向量中，被广泛应用于LLaMA、ChatGLM等现代大语言模型中，具有良好的外推性。
     - **NTK-aware Scaled RoPE**：通过神经切线核理论对RoPE进行插值，可以在不微调的情况下有效扩展上下文长度。

   - **高效注意力机制**：如Longformer、BigBird等，通过稀疏化注意力模式（如局部窗口注意力+全局注意力），将复杂度从 O(n2)O(n2) 降低到 O(n)O(n) 或 O(nlog⁡n)O(nlogn)，从而支持更长的序列。


**结论：**

- **理论上**，使用类似正弦编码的Transformer不能无限长推理，因为模型权重未在超长序列上训练。
- **实际上**，输入长度受限于**模型的训练长度**和**硬件的计算能力**。
- **研究前沿**：通过改进位置编码（如RoPE, ALiBi）和注意力机制，我们正在不断突破上下文长度的上限（从早期的2K到现在的1M甚至更长），但“无限长”仍然是一个遥不可及的目标。









# to read

[https://zhuanlan.zhihu.com/p/454482273](https://zhuanlan.zhihu.com/p/454482273)