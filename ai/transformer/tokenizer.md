# tokenizer

把一段文字变成一组token，也就是词元化 tokenization； openai 的token是按照偏旁部首拆分，unicode编码？

openai开源 tiktoken

- 2020年GPT3 token,词表 50257个token
- 2023年GPT4 token ,词表：100256 个token
- 2024年Llama3 token词表: 128000 token

token化非常预留减少词表的数量( token数量少于文字的总数)

# embedding

大模型包含了  vocabulary  * <span style="color:rgba(16,185,129,1)">   </span>$d_{model}$ (模型向量维度)

# positional encoding

# 编码和解码器

# 线性和softmax







sh