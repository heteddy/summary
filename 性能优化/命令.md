# vmstat

vmstat 是一个很全面的性能分析工具，可以观察到系统的进程状态、内存使用、虚拟内存使用、磁盘的IO、中断、上下文切换、CPU使用等。对于 Linux 的性能分析，100%理解 vmstat 输出内容的含义，并能灵活应用，那对系统性能分析的能力就算是基本掌握了。

>   vmstat -S m 1 5 ， 以m为单位
>
>   `-a` 参数 活动和非活动内存
>
>   `-d` 参数会以每个磁盘一行的方式显示统计（包含读、写和 IO）
>
>   `-p` 参数后面跟上设备名会显示指定分区统计
>
>   `-D` 会显示全局统计（包括全部的磁盘、分区、全部读、合并的读、读取的扇区、写、合并的写、写入的扇区和 IO）。
>
>   `-m` 参数会显示 slab 信息。
>
>   `-s` 参数会显示不同统计的总结。

![](https://upload-images.jianshu.io/upload_images/9243349-627d257868eb2545.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

1）procs

+   r列表示运行和等待CPU时间片的进程数，这个值如果长期大于系统CPU个数，就说明CPU资源不足，可以考虑增加CPU；

+   b列表示在等待资源的进程数，比如正在等待I/O或者内存交换等。

2）memory

+   swpd列表示切换到内存交换区的内存数量（以KB为单位）。**如果swpd的值不为0或者比较大，而且si、so的值长期为0，那么这种情况一般不用担心，不会影响系统性能**；

    >   swapon -s命令查看当前系统上正在使用的交换空间有哪些，以及相关信息

+   free列表示当前空闲的物理内存数量（以KB为单位）；

+   buff列表示buffers cache的内存数量，一般对块设备的读写才需要缓冲；

+   cache列表示page cached的内存数量，一般作文件系统的cached，频繁访问的文件都会被cached。如果cached值较大，就说明cached文件数较多。如果此时IO中的bi比较小，就说明文件系统效率比较好。

3）swap

+   si列表示由磁盘调入内存，也就是内存进入内存交换区的数量；从磁盘交换的内存量（换入，从 swap 移到实际内存的内存）

+   so列表示由内存调入磁盘，也就是内存交换区进入内存的数量

+   一般情况下，si、so的值都为0，如果si、so的值长期不为0，则表示系统内存不足，需要考虑是否增加系统内存。

4）IO

+   bi列表示从块设备读入的数据总量（即读磁盘，单位KB/秒）

+   bo列表示写入到块设备的数据总量（即写磁盘，单位KB/秒）

这里设置的bi+bo参考值为1000，如果超过1000，而且wa值比较大，则表示系统磁盘IO性能瓶颈。

5）system

+   in列表示在某一时间间隔中观察到的每秒设备中断数；

+   cs列表示每秒产生的上下文切换次数。

上面这两个值越大，会看到内核消耗的CPU时间就越多。

6）CPU

+   us列显示了用户进程消耗CPU的时间百分比。us的值比较高时，说明用户进程消耗的CPU时间多，如果长期大于50%，需要考虑优化程序啥的。

+   sy列显示了内核进程消耗CPU的时间百分比。sy的值比较高时，就说明内核消耗的CPU时间多；如果us+sy超过80%，就说明CPU的资源存在不足。

+   id列显示了CPU处在空闲状态的时间百分比；

+   wa列表示IO等待所占的CPU时间百分比。wa值越高，说明IO等待越严重。如果wa值超过20%，说明IO等待严重。

+   st列一般不关注，虚拟机占用的时间百分比。

# pidstat

>   pidstat 查看每个进程的详细情况   
>
>   -d     Report I/O statistics
>
>   -u     Report CPU utilization.
>
>   -w     Report task switching activity
>
>   -p  { pid [,...] | SELF | ALL }

查看单个进程的pidstat -w 1

```bash
[root@education_01 ~]# pidstat -w 1
Linux 3.10.0-957.27.2.el7.x86_64 (education_01) 	03/15/2021 	_x86_64_	(4 CPU)

05:56:37 PM   UID       PID   cswch/s nvcswch/s  Command
05:56:38 PM     0         9     69.00      0.00  rcu_sched
05:56:38 PM     0       411      9.00      0.00  kworker/1:1H
05:56:38 PM     0       412     10.00      0.00  jbd2/vda1-8
05:56:38 PM  1000       888      1.00      0.00  uwsgi
05:56:38 PM     0      1467     10.00      0.00  redis-server
05:56:38 PM     0      4305      2.00      0.00  docker-proxy
05:56:38 PM     0      4333      1.00      0.00  tini
05:56:38 PM     0      4465      1.00      0.00  sshd
05:56:38 PM  1000      4696      3.00      0.00  node
05:56:38 PM     0      6361      1.00      0.00  kworker/u8:2
05:56:38 PM     0      8604      1.00      0.00  kworker/0:1
05:56:38 PM     0     10913      3.00      0.00  kworker/3:1
05:56:38 PM     0     13680      1.00      0.00  kworker/1:2
05:56:38 PM     0     13694      1.00      0.00  pidstat
05:56:38 PM     0     21874      3.00      0.00  kworker/1:0
05:56:38 PM     0     28817      1.00      0.00  kworker/2:0
05:56:38 PM   472     31268      1.00      0.00  grafana-server
```

cswch ，表示每秒自愿上下文切换(voluntary context switches）的次数，

nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数

+   自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说，I/O、内存等系统资源不足时，就会发生自愿上下文切换。

+   非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，**大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换**

监控指定的进程

```bash
[root@education_01 ~]# pidstat -p 888 -w 3
Linux 3.10.0-957.27.2.el7.x86_64 (education_01) 	03/16/2021 	_x86_64_	(4 CPU)

09:26:54 AM   UID       PID   cswch/s nvcswch/s  Command
09:26:57 AM  1000       888      1.00      0.00  uwsgi
09:27:00 AM  1000       888      1.00      0.00  uwsgi
09:27:03 AM  1000       888      1.00      0.00  uwsgi
09:27:06 AM  1000       888      1.00      0.00  uwsgi
09:27:09 AM  1000       888      1.00      0.00  uwsgi
09:27:12 AM  1000       888      1.00      0.00  uwsgi
```



# iostat

>   iostat - Report Central Processing Unit (CPU) statistics and input/output statistics for devices and partitions.

## 数据来源

iostat数据的来源是Linux操作系统的/proc/diskstats

procfs中的前三个字段：主设备号、从设备号、设备名。

![](https://img-blog.csdnimg.cn/20200221235944110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Nvb2Rlcl9TWEs=,size_16,color_FFFFFF,t_70)

从第四个字段开始，介绍的是该设备的相关统计：

-   (rd_ios) : 读操作的次数
-   (rd_merges):合并读操作的次数。如果两个读操作读取相邻的数据块，那么可以被合并成1个。
-   (rd_sectors): 读取的扇区数量
-   (rd_ticks):读操作消耗的时间（以毫秒为单位）。每个读操作从__make_request()开始计时，到end_that_request_last()为止，包括了在队列中等待的时间。
-   (wr_ios):写操作的次数
-   (wr_merges):合并写操作的次数
-   (wr_sectors): 写入的扇区数量
-   (wr_ticks): 写操作消耗的时间（以毫秒为单位）
-   (in_flight): 当前未完成的I/O数量。在I/O请求进入队列时该值加1，在I/O结束时该值减1。 注意：是I/O请求进入队列时，而不是提交给硬盘设备时
-   (io_ticks)该设备用于处理I/O的自然时间(wall-clock time)
-   (time_in_queue): 对字段#10(io_ticks)的加权值



参数：

```bash
# 显示M/s
-m     Display statistics in megabytes per second.
# 显示时间
-t     Print the time for each report displayed. The timestamp format may depend on the value of the S_TIME_FORMAT environment variable (see below).
# 显示扩展信息
-x     Display extended statistics.
```

```
[root@education_01 ~]# iostat -mtx 2
Linux 3.10.0-957.27.2.el7.x86_64 (education_01) 	03/16/2021 	_x86_64_	(4 CPU)

03/16/2021 10:58:45 AM
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.73    0.00    0.61    0.07    0.00   98.59

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
vda               0.00     6.93    0.06    4.69     0.00     0.05    20.85     0.01    3.42   19.51    3.22   0.29   0.14
vdb               0.00     2.88    0.00    1.09     0.00     0.02    32.20     0.00    1.70    4.55    1.69   0.82   0.09
scd0              0.00     0.00    0.00    0.00     0.00     0.00     7.93     0.00    0.21    0.21    0.00   0.21   0.00

03/16/2021 10:58:47 AM
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           4.14    0.00    0.25    0.00    0.00   95.61

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
vda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
vdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
scd0              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00

```

第一列Device比较容易理解，就是说这一行描述的是哪一个设备。

-   rrqm/s : 每秒合并读操作的次数(The number of read requests merged per second that were queued to the device.)
-   wrqm/s: 每秒合并写操作的次数
-   r/s ：每秒读操作的次数
-   w/s : 每秒写操作的次数
-   rMB/s :每秒读取的MB字节数
-   wMB/s: 每秒写入的MB字节数
-   avgrq-sz：每个IO的平均扇区数，即所有请求的平均大小，以扇区（512字节）为单位(The average size (in sectors) of the requests that were issued to the device.)
-   avgqu-sz：平均未完成的IO请求数量，即平均意义上的请求队列长度(The average queue length of the requests that were issued to the device.)
-   await：平均每个IO所需要的时间，包括在队列等待的时间，也包括磁盘控制器处理本次请求的有效时间。The average time (in milliseconds) for I/O requests issued to the device to be served. This includes the  time  spent  by  the requests in queue and the time spent servicing them.
-   r_await：每个读操作平均所需要的时间，不仅包括硬盘设备读操作的时间，也包括在内核队列中的时间。
-   w_await: 每个写操平均所需要的时间，不仅包括硬盘设备写操作的时间，也包括在队列中等待的时间。
-   svctm： 表面看是每个IO请求的服务时间，不包括等待时间，但是实际上，这个指标已经废弃。实际上，iostat工具没有任何一输出项表示的是硬盘设备平均每次IO的时间。 The  average  service time (in milliseconds) for I/O requests that were issued to the device. **Warning! Do not trust this field any more.**  This field will be removed in a future sysstat version.
-   %util： 工作时间或者繁忙时间占总时间的百分比Percentage of elapsed time during which I/O requests were issued to the device (bandwidth utilization for the device).  Device saturation occurs when this value is close to 100%



## 例子

### avgqu-sz

+   fio  iodepth =1 可以看到avgqu-sz也是1

```bash
fio --name=randwrite --rw=randwrite --bs=4k --size=20G --runtime=1200 --ioengine=libaio --iodepth=1 --numjobs=1 --filename=/dev/sdc --direct=1 --group_reporting
```

![](https://img-blog.csdnimg.cn/20200221235327304.png)

+   fio iodepth =16，avgqu-sz也是16

```
fio --name=randwrite --rw=randwrite --bs=4k --size=20G --runtime=1200 --ioengine=libaio --iodepth=16 --numjobs=1 --filename=/dev/sdc --direct=1 --group_reporting 
```

![](https://img-blog.csdnimg.cn/20200221235425585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Nvb2Rlcl9TWEs=,size_16,color_FFFFFF,t_70)

内核中有I/O Scheduler队列。我们看到因为avgqu-sz大小不一样，所以一个IO时间（await）就不一样。

>   上图中的avgrq-sz都是8 ，这是因为blocksize bs=4k，因此是8个扇区

### avgrq-sz

avgrq-sz这个值反应了用户的IO-Pattern。我们经常关心，用户过来的IO是大IO还是小IO，那么avgrq-sz反应了这个要素。它的含义是说，平均下来，这这段时间内，所有请求的平均大小，单位是扇区，即（512字节）。

```bash
fio --name=randwrite --rw=randwrite --bs=128k --size=20G --runtime=1200 --ioengine=libaio --iodepth=1 --numjobs=1 --filename=/dev/sdc --direct=1 --group_reporting 
```

![](https://img-blog.csdnimg.cn/20200221235524700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Nvb2Rlcl9TWEs=,size_16,color_FFFFFF,t_70)

意sdc的avgrq-sz这列的值，变成了256，即256 个扇区 = 256* 512 Byte = 128KB，等于我们fio测试时，下达的bs = 128k。

>   注意，这个值也不是为所欲为的，它受内核参数的控制：
>
>   root@node-186:~# cat  /sys/block/sdc/queue/max_sectors_kb  
>
>   256

这个值不是最大下发的IO是256KB，即512个扇区。当我们fio对sdc这块盘做测试的时候，如果bs=256k，iostat输出中的avgrq-sz 会变成 512 扇区，但是，如果继续增大bs，比如bs=512k，那么iostat输出中的avgrq-sz不会继续增大，仍然是512，表示512扇区。

### rrqm/s 和wrqm/s

块设备有相应的调度算法。如果两个IO发生在相邻的数据块时，他们可以合并成1个IO。

设备vdb 改成指定的设备

```
[root@education_01 ~]# cat /sys/block/vdb/queue/scheduler
[mq-deadline] kyber none
```

## 原理

iostat数据的来源是Linux操作系统的/proc/diskstats

procfs中的前三个字段：主设备号、从设备号、设备名。

![](https://img-blog.csdnimg.cn/20200221235944110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Nvb2Rlcl9TWEs=,size_16,color_FFFFFF,t_70)

/proc/diskstats

-   (rd_ios) : 读操作的次数
-   (rd_merges):合并读操作的次数。如果两个读操作读取相邻的数据块，那么可以被合并成1个。
-   (rd_sectors): 读取的扇区数量
-   (rd_ticks):读操作消耗的时间（以毫秒为单位）。每个读操作从__make_request()开始计时，到end_that_request_last()为止，包括了在队列中等待的时间。
-   (wr_ios):写操作的次数
-   (wr_merges):合并写操作的次数
-   (wr_sectors): 写入的扇区数量
-   (wr_ticks): 写操作消耗的时间（以毫秒为单位）
-   (in_flight): 当前未完成的I/O数量。在I/O请求进入队列时该值加1，在I/O结束时该值减1。 注意：是I/O请求进入队列时，而不是提交给硬盘设备时
-   (io_ticks)该设备用于处理I/O的自然时间(wall-clock time)
-   (time_in_queue): 对字段#10(io_ticks)的加权值



```c
struct disk_stats {
	unsigned long sectors[2];       /* READs and WRITEs */
	unsigned long ios[2];
	unsigned long merges[2];
    unsigned long ticks[2];
    unsigned long io_ticks;
    unsigned long time_in_queue;
};
```

### inflight

当前未完成的I/O数量。在I/O请求进入队列时该值加1，在I/O结束时该值减1。 注意：是I/O请求进入队列时，而不是提交给硬盘设备时

```c
part_in_flight(hd), 
static inline int part_in_flight(struct hd_struct *part)
{
        return atomic_read(&part->in_flight[0]) + atomic_read(&part->in_flight[1]);
}
```

```c
while ((hd = disk_part_iter_next(&piter))) {
  cpu = part_stat_lock();
  part_round_stats(cpu, hd);
  part_stat_unlock();
  seq_printf(seqf, "%4d %7d %s %lu %lu %llu "
         "%u %lu %lu %llu %u %u %u %u\n",
         MAJOR(part_devt(hd)), MINOR(part_devt(hd)),
         disk_name(gp, hd->partno, buf),
         part_stat_read(hd, ios[READ]),
         part_stat_read(hd, merges[READ]),
         (unsigned long long)part_stat_read(hd, sectors[READ]),
         jiffies_to_msecs(part_stat_read(hd, ticks[READ])),
         part_stat_read(hd, ios[WRITE]),
         part_stat_read(hd, merges[WRITE]),
         (unsigned long long)part_stat_read(hd, sectors[WRITE]),
         jiffies_to_msecs(part_stat_read(hd, ticks[WRITE])),
         part_in_flight(hd),
         jiffies_to_msecs(part_stat_read(hd, io_ticks)),
         jiffies_to_msecs(part_stat_read(hd, time_in_queue))
      );
```

### io_ticks

这里面大部分字段都是很容易理解的，稍微难理解的在于io_ticks。初看之下，明明已经有了rd_ticks和wr_ticks 为什么还需一个io_ticks。注意rd_ticks和wr_ticks是把每一个IO消耗时间累加起来，但是硬盘设备一般可以并行处理多个IO，因此，rd_ticks和wr_ticks之和一般会比自然时间（wall-clock time）要大。而**io_ticks 不关心队列中有多少个IO在排队，它只关心设备有IO的时间。即不考虑IO有多少，只考虑IO有没有**。在实际运算中，in_flight不是0的时候保持计时，而**in_flight 等于0的时候，时间不累加到io_ticks**。

下一个比较难理解的是time_in_queue这个值，它的计算是当前IO数量（即in_flight的值）乘以自然时间间隔。表面看该变量的名字叫time_in_queue，但是实际上，并不只是在队列中等待的时间。

### time_in_queue

```c
static void part_round_stats_single(int cpu, struct hd_struct *part, unsigned long now)
{
  if (now == part->stamp)
      return;
   
  /*如果队列不为空，存在in_flight io*/
  if (part_in_flight(part)) {
     
      /*小学数学老师的算法，now-part->stamp 乘以班级人数，哦不，是乘以队列中等待的io请求个数*/
      __part_stat_add(cpu, part, time_in_queue,
              part_in_flight(part) * (now - part->stamp));
     
     /*如实的记录，因为批评调皮学生，浪费了5分钟。io不是空的时间增加now - part->stamp*/
      __part_stat_add(cpu, part, io_ticks, (now - part->stamp));
  }
  part->stamp = now;
}
```

这个计算的方法很简单：

-   当请求队列为空的时候：
    -   io_ticks不增加
    -   time_in_queue不增加
    -   part->stamp 更新为now
-   当请求队列不是空的时候：
    -   io_ticks增加， 增加量为 now - part->timestamp
    -   time_in_queue增加，增加量为 在队列中IO的个数乘以 (now - part->stamp)
    -   part->stamp 更新为now

注意调用part_round_stats_single函数的时机在于：

-   在新IO请求插入队列（被merge的不算）

-   完成一个IO请求
    空说太过抽象，但是我们还是给出一个例子来介绍io_ticks和time_in_queue的计算：

    ~~一下表格有问题~~

| ID   |  Time  | Ops            | in_flight | stamp  | stamp_delta           | io_ticks            | time_in_queue  |
| ---- | :----: | -------------- | :-------: | ------ | --------------------- | ------------------- | -------------- |
| 0    |  100   | 新请求入队列   |     0     | 0      | 无需计算              | 0                   | 0              |
| 1    | 100.10 | 新请求入队列   |     1     | 100    | 100.10-100 = 0.1      | 0.1                 | 0.1            |
| 2    | 101.20 | 完成一个IO请求 |     2     | 100.10 | 101.20-100.10 = 1.1   | 1.2 0.1+1.1*2 = 2.3 |                |
| 3    | 103.60 | 完成一个IO请求 |     1     | 101.20 | 103.60-101.20 = 2.4   | 3.6                 | 2.3+2.4*1=4.7  |
| 4    | 153.60 | 新请求入队列   |     0     | 103.60 | 无需计算              | 3.6                 | 4.7            |
| 5    | 153.90 | 完成一个IO请求 |     1     | 153.60 | 153.90 - 153.60 = 0.3 | 3.9                 | 4.7+0.3 * 1= 5 |



注意上面总时间是53.90时间内，有3.9秒的自然时间内是有IO的，即IO队列的非空时间为3.9秒。

注意，io_ticks这个字段被iostat用来计算%util，而time_in_queue这个字段被iostat用来计算avgqu-sz，即平均队列长度。

其实不难理解了，队列中不为空的时候占总时间的比例即为 %util



### rd_ios/wr_ios  rd_ticks/wr_tick

在每个IO结束后，都会调用blk_account_io_done函数，这个函数会负责更新rd_ios/wr_ios、rd_ticks/wr_ticks ,包括会更新in_flight。

```c
void blk_account_io_done(struct request *req)
{
        /*   
         * Account IO completion.  flush_rq isn't accounted as a
         * normal IO on queueing nor completion.  Accounting the
         * containing request is enough.
         */
        if (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) {
                unsigned long duration = jiffies - req->start_time;
                /*从req获取请求类型：R / W*/
                const int rw = rq_data_dir(req);
                struct hd_struct *part;
                int cpu; 

                cpu = part_stat_lock();
                part = req->part;
               /*更新读或写次数，自加*/
                part_stat_inc(cpu, part, ios[rw]);
                /*将io的存活时间，更新到rd_ticks or wr_ticks*/
                part_stat_add(cpu, part, ticks[rw], duration);
                /*更新io_ticks和time_in_queue*/
                part_round_stats(cpu, part);
                /*对应infight 减 1 */
                part_dec_in_flight(part, rw); 

                hd_struct_put(part);
                part_stat_unlock();
        }                                                                                                                                              
}
```



关于merge部分的统计，在blk_account_io_start函数中统计：

![](https://img-blog.csdnimg.cn/20200222001202625.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Nvb2Rlcl9TWEs=,size_16,color_FFFFFF,t_70)

```c
void blk_account_io_start(struct request *rq, bool new_io)
{
        struct hd_struct *part;
        int rw = rq_data_dir(rq);                                             
        int cpu;
        
        if (!blk_do_io_stat(rq))
                return;
                
        cpu = part_stat_lock();
        
        if (!new_io) {
                /*注意，merge的IO就不会导致in_flight++*/
                part = rq->part;
                part_stat_inc(cpu, part, merges[rw]);
        } else {
                part = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));
                if (!hd_struct_try_get(part)) {
                        part = &rq->rq_disk->part0;
                        hd_struct_get(part);
                }
                /*新IO，更新io_ticks and time_in_queue*/
                part_round_stats(cpu, part);
                /*in_flight 加1*/
                part_inc_in_flight(part, rw);
                rq->part = part;
        }
        
        part_stat_unlock();
}   
```

### iostat的源码

iostat的源码非常的短，它属于sysstat这个开源软件，整个文件大小1619行

```c
int read_sysfs_file_stat(int curr, char *filename, char *dev_name)
{
        FILE *fp; 
        struct io_stats sdev;
        int i;
        unsigned int ios_pgr, tot_ticks, rq_ticks, wr_ticks;
        unsigned long rd_ios, rd_merges_or_rd_sec, wr_ios, wr_merges;
        unsigned long rd_sec_or_wr_ios, wr_sec, rd_ticks_or_wr_sec;

        /* Try to read given stat file */
        if ((fp = fopen(filename, "r")) == NULL)
                return 0;

        i = fscanf(fp, "%lu %lu %lu %lu %lu %lu %lu %u %u %u %u",
                   &rd_ios, &rd_merges_or_rd_sec, &rd_sec_or_wr_ios, &rd_ticks_or_wr_sec,
                   &wr_ios, &wr_merges, &wr_sec, &wr_ticks, &ios_pgr, &tot_ticks, &rq_ticks);

        if (i == 11) {
                /* Device or partition */
                sdev.rd_ios     = rd_ios;
                sdev.rd_merges  = rd_merges_or_rd_sec;
                sdev.rd_sectors = rd_sec_or_wr_ios;
                sdev.rd_ticks   = (unsigned int) rd_ticks_or_wr_sec;
                sdev.wr_ios     = wr_ios;
                sdev.wr_merges  = wr_merges;                               
                sdev.wr_sectors = wr_sec;
                sdev.wr_ticks   = wr_ticks;
                sdev.ios_pgr    = ios_pgr;
                sdev.tot_ticks  = tot_ticks;
                sdev.rq_ticks   = rq_ticks;
        }
        else if (i == 4) {
                /* Partition without extended statistics */
                sdev.rd_ios     = rd_ios;
                sdev.rd_sectors = rd_merges_or_rd_sec;
                sdev.wr_ios     = rd_sec_or_wr_ios;
                sdev.wr_sectors = rd_ticks_or_wr_sec;
        }
        if ((i == 11) || !DISPLAY_EXTENDED(flags)) {
                /*
                 * In fact, we _don't_ save stats if it's a partition without
                 * extended stats and yet we want to display ext stats.
                 */
                save_stats(dev_name, curr, &sdev, iodev_nr, st_hdr_iodev);
        }

        fclose(fp);
        
        return 1;
} 
```

数据都采集到了，剩下就是计算了。其中下面几项的计算是非常简单的：

-   wrqm/s
-   r/s
-   w/s
-   rMB/s
-   wMB/s
    这几项的计算是非常简单的，就是采样两次，后一次的值减去前一次的值，然后除以时间间隔，得到平均值即可。因为这些/proc/diskstats中对应的值都是累加的，后一次减去前一次，即得到采样时间间隔内的新增量。

### avgrq-sz的计算

```c
 /*       rrq/s wrq/s   r/s   w/s  rsec  wsec  rqsz  qusz await r_await w_await svctm %util */
        cprintf_f(2, 8, 2,
                  S_VALUE(ioj->rd_merges, ioi->rd_merges, itv),
                  S_VALUE(ioj->wr_merges, ioi->wr_merges, itv));
        cprintf_f(2, 7, 2,
                  S_VALUE(ioj->rd_ios, ioi->rd_ios, itv),
                  S_VALUE(ioj->wr_ios, ioi->wr_ios, itv));
        cprintf_f(4, 8, 2,
                  S_VALUE(ioj->rd_sectors, ioi->rd_sectors, itv) / fctr,
                  S_VALUE(ioj->wr_sectors, ioi->wr_sectors, itv) / fctr,
                  xds.arqsz,  //此处是avgrq-sz
                  S_VALUE(ioj->rq_ticks, ioi->rq_ticks, itv) / 1000.0);//此处是avgqu-sz
```

注意avgrq-sz来自xds的argsz变量，该变量是通过该函数计算得到的：

```c
/*注意sdc中的c指的是current，sdp中的p指的是previous*/
void compute_ext_disk_stats(struct stats_disk *sdc, struct stats_disk *sdp,
                            unsigned long long itv, struct ext_disk_stats *xds)
{
        double tput
                = ((double) (sdc->nr_ios - sdp->nr_ios)) * HZ / itv;

        xds->util  = S_VALUE(sdp->tot_ticks, sdc->tot_ticks, itv);
        xds->svctm = tput ? xds->util / tput : 0.0;
        xds->await = (sdc->nr_ios - sdp->nr_ios) ?
                ((sdc->rd_ticks - sdp->rd_ticks) + (sdc->wr_ticks - sdp->wr_ticks)) /
                ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0;
        
        xds->arqsz = (sdc->nr_ios - sdp->nr_ios) ?
                ((sdc->rd_sect - sdp->rd_sect) + (sdc->wr_sect - sdp->wr_sect)) /
                ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0;
}
```

### nr_ios来自如下运算，即读IO和写IO的和

```c
sdc.nr_ios = ioi->rd_ios + ioi->wr_ios;
sdp.nr_ios = ioj->rd_ios + ioj->wr_ios;
```

### xds->arqsz 的计算

平均读写扇区

```c
ds->arqsz = (读扇区总数 + 写扇区总数)/(读IO次数+写IO次数)
xds->arqsz = (sdc->nr_ios - sdp->nr_ios) ?
            ((sdc->rd_sect - sdp->rd_sect) + (sdc->wr_sect - sdp->wr_sect)) /
            ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0;
```

### await、r_wait及w_wait的计算

await  = IO 平均处理时间 + IO在队列的平均等待时间

```C
void compute_ext_disk_stats(struct stats_disk *sdc, struct stats_disk *sdp,
                            unsigned long long itv, struct ext_disk_stats *xds)
{
        ...
        xds->await = (sdc->nr_ios - sdp->nr_ios) ?
                ((sdc->rd_ticks - sdp->rd_ticks) + (sdc->wr_ticks - sdp->wr_ticks)) /
                ((double) (sdc->nr_ios - sdp->nr_ios)) : 0.0; 
        ...
}
```

